{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "______________________________________\n",
    "# <center>**Trabajo Practico N¬∫2 para la Materia *Organizaci√≥n de Datos***</center>\n",
    "\n",
    "*Integrantes*:\n",
    "- 103963\tCarolina Di Matteo\tcdimatteo@fi.uba.ar\n",
    "- 101231\tPablo Salvador Dimartino\tpdimartino@fi.uba.ar\n",
    "- 100113\tJuan Sebastian Burgos\tjsburgos@fi.uba.ar\n",
    "- 104415\tValentina Laura Correa\tvcorrea@fi.uba.ar\n",
    "\n",
    "*Grupo*: 14\n",
    "\n",
    "*Repositorio*: [github](https://github.com/valencorrea/7506R-2C2022-GRUPO14)\n",
    "\n",
    "*Curso*: Rodriguez\n",
    "\n",
    "*Cuatrimestre*: 2c2022\n",
    "\n",
    "Datos provistos por [properati](https://www.properati.com.ar).\n",
    "______________________________________\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "ALhQpMoS2Do2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introducci√≥n"
   ],
   "metadata": {
    "id": "jrObdaVTw13E"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "El presente trabajo pr√°ctico es una continuaci√≥n del ‚ÄòTP1: Propiedades en Venta‚Äô. \n",
    "\n",
    "En la entrega anterior se propuso aplicar t√©cnicas de an√°lisis exploratorio, preprocesamiento de datos, agrupamiento, clasificaci√≥n y regresi√≥n. Siguiendo esta l√≠nea, y con el objetivo de continuar resolviendo problemas reales de ciencia de datos, en esta segunda parte se implementar√°n nuevos modelos predictivos a partir de los anteriormente mencionados. \n",
    "\n",
    "En esta oportunidad se buscar√° demostrar los conocimientos adquiridos sobre procesamiento del lenguaje natural, redes neuronales y ensamble de modelos. Para esto se utilizar√°n tanto datasets provistos por la materia, tomados de la p√°gina de la empresa Properati, como los generados por el grupo en el trabajo anterior."
   ],
   "metadata": {
    "id": "8Tot765gvLP2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Setup"
   ],
   "metadata": {
    "id": "civIgEQKvQ6A"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Importaci√≥n de bibliotecas"
   ],
   "metadata": {
    "id": "vrqonJJjvVay"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pip install visualkeras scikeras[tensorflow] scikeras[tensorflow-cpu] unidecode types-all tensorflow keras"
   ],
   "metadata": {
    "id": "h6RrCiyq2Do7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3e7b3e93-b112-4351-a933-5d45f291d371"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Importaci√≥n de librer√≠as\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from collections import Counter\n",
    "import unidecode\n",
    "import re\n",
    "from joblib import load, dump\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import recall_score, accuracy_score, f1_score, precision_score, r2_score\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "#Configuraci√≥n de Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "#Ejecuci√≥n con Drive\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    IN_COLAB = True\n",
    "else:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB :\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    properati = pd.read_csv('/content/drive/MyDrive/üìî OrganizacioÃÅn de Datos (75.06)/TPS/TP2/1d_df_reducido.csv')\n",
    "    properati_descrip = pd.read_csv('/content/drive/MyDrive/üìî OrganizacioÃÅn de Datos (75.06)/TPS/TP1/properati_argentina_2021_decrip.csv')\n",
    "    stop_words = '/content/drive/MyDrive/üìî OrganizacioÃÅn de Datos (75.06)/TPS/TP2/stopwords.txt'\n",
    "else:\n",
    "    df_train = pd.read_csv('./DATASETS/df_train_tp1.csv')\n",
    "    df_test = pd.read_csv('./DATASETS/df_test_tp1.csv')\n",
    "    properati_descrip = pd.read_csv('properati_argentina_2021_decrip.csv')\n",
    "    stop_words = 'stopwords'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Funciones auxiliares"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def metricas_clasificacion(y_test, y_pred):\n",
    "    print(f'Accuracy: {round(accuracy_score(y_test, y_pred),2)}')\n",
    "    print(f'Precision: {round(precision_score(y_test, y_pred, average=\"macro\"),2)}')\n",
    "    print(f'Recall: {round(recall_score(y_test, y_pred, average=\"macro\"),2)}')\n",
    "    print(f'F1 Score: {round(f1_score(y_test, y_pred, average=\"macro\"),2)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def prediccion_y_metricas_regresion(regressor, x_test, y_test):\n",
    "\n",
    "  y_pred = regressor.predict(x_test)\n",
    "\n",
    "  print(f\"Se obtuvo un Score de {round(regressor.score(x_test, y_test)*100,3)}%\")\n",
    "\n",
    "  mse = metrics.mean_squared_error(\n",
    "        y_true  = y_test,\n",
    "        y_pred  = y_pred,\n",
    "        squared = True\n",
    "       )\n",
    "\n",
    "  print(f\"El error seg√∫n la m√©trica 'Mean Square Error' de test es: {mse}\")\n",
    "\n",
    "  rmse = metrics.mean_squared_error(\n",
    "        y_true  = y_test,\n",
    "        y_pred  = y_pred,\n",
    "        squared = False\n",
    "       )\n",
    "\n",
    "  print(f\"El error seg√∫n la m√©trica 'Root Mean Square Error' de test es: {rmse}\")\n",
    "\n",
    "  return"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def format_aspects(line, word):\n",
    "    format = r\"\\W*([\\w]+)\"\n",
    "    n = 2\n",
    "    x = re.search(r'{}\\W*{}{}'.format(format*n, word, format*n), line)\n",
    "    if x is not None:\n",
    "        return x.group()\n",
    "    else:\n",
    "        return \"\""
   ],
   "metadata": {
    "id": "CCKsO-A5w7TQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def convert_b_m_a(x):\n",
    "    mx = max(x[0], x[1], x[2])\n",
    "    if mx == x[0]:\n",
    "        return 'bajo'\n",
    "    elif mx == x[1]:\n",
    "        return 'medio'\n",
    "    elif mx == x[2]:\n",
    "        return 'alto'"
   ],
   "metadata": {
    "id": "3suAr0ciwY91"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def estandarizar(df, columns):\n",
    "  sscaler = StandardScaler()\n",
    "\n",
    "  for col in columns:\n",
    "    df[col] = sscaler.fit_transform(pd.DataFrame(df[col]))"
   ],
   "metadata": {
    "id": "0LF1eyB9l4nt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def limpiar_values_de_aspects(df, aspects, values):\n",
    "    i = 0\n",
    "    for aspect in aspects:\n",
    "        for word in values[i]:\n",
    "            df[aspect] = df[aspect].apply(lambda line: word if word in line else line)\n",
    "        df[aspect] = df[aspect].apply(lambda line: line if len(line.split())<2 else '')\n",
    "        i = i+1"
   ],
   "metadata": {
    "id": "AAEy2PV92DpO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_model(model, x, y, splits, n):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=splits, n_repeats=n, random_state=1)\n",
    "    scores = cross_val_score(model, x, y, scoring='neg_mean_squared_error', cv=cv, verbose=1, n_jobs=3, error_score='raise')\n",
    "    return scores"
   ],
   "metadata": {
    "id": "tDt5-bPcEn_m"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_results(model_scores, name):\n",
    "    \n",
    "    model_names = list(model_scores.keys())\n",
    "    results = [model_scores[model] for model in model_names]\n",
    "    fig = go.Figure()\n",
    "    for model, result in zip(model_names, results):\n",
    "        fig.add_trace(go.Box(\n",
    "            y=result,\n",
    "            name=model,\n",
    "            boxpoints='all',\n",
    "            jitter=0.5,\n",
    "            whiskerwidth=0.2,\n",
    "            marker_size=2,\n",
    "            line_width=1)\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(\n",
    "    title='Performance of Different Models Using 5-Fold Cross-Validation',\n",
    "    paper_bgcolor='rgb(243, 243, 243)',\n",
    "    plot_bgcolor='rgb(243, 243, 243)',\n",
    "    xaxis_title='Model',\n",
    "    yaxis_title='Error Cuadr√°tico Medio',\n",
    "    showlegend=False)\n",
    "    fig.show()"
   ],
   "metadata": {
    "id": "NImxsouY4HaJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preparaci√≥n de datasets"
   ],
   "metadata": {
    "id": "yG33nESmvkqN"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_train_x = df_train.drop([\"property_price\", \"tipo_precio_3\"], axis=\"columns\")\n",
    "df_train_y_regresion = df_train[\"property_price\"]\n",
    "df_train_y_clasificacion = df_train[\"tipo_precio_3\"]\n",
    "\n",
    "df_test_x = df_test.drop([\"property_price\", \"tipo_precio_3\"], axis=\"columns\")\n",
    "df_test_y_regresion = df_test[\"property_price\"]\n",
    "df_test_y_clasificacion = df_test[\"tipo_precio_3\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Procesamiento del Lenguaje Natural"
   ],
   "metadata": {
    "collapsed": false,
    "id": "DajLENWv2DpA"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.a Ampliaci√≥n del dataset\n",
    "___"
   ],
   "metadata": {
    "id": "mmjJ3qskuJyJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hacemos un merge del dataset original y el de descripciones, y qued√©monos √∫nicamente con las columnas `id` y `property_description`:"
   ],
   "metadata": {
    "id": "904YC-oLyddb"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_descrip = pd.merge(df_train_x, properati_descrip, on=\"id\")\n",
    "df_descrip = df_descrip[[\"id\", \"property_description\"]]\n",
    "\n",
    "df_test_descrip = pd.merge(df_test_x, properati_descrip, on=\"id\")\n",
    "df_test_descrip = df_test_descrip[[\"id\", \"property_description\"]]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "vRbU2jpk2Do_",
    "outputId": "44fcabfb-0e8b-496a-881f-0de5fbc8f3e6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_test_descrip"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Busquemos aspectos de una propiedad utilizando la columna `property_description`."
   ],
   "metadata": {
    "id": "uCEL8YyUxYaa"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Veamos cu√°ntos registros nulos existen:"
   ],
   "metadata": {
    "id": "tAEE-G_QzCXj"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Hay {df_descrip['property_description'].isna().sum()} datos nulos.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ydHTArN62DpA",
    "outputId": "532aec7a-b0bc-4f97-d56d-80ee53135a25"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Veamos cu√°les son las 100 palabras m√°s comunes en el campo de descripci√≥n de propiedades:"
   ],
   "metadata": {
    "id": "t4hiRECrz3Wa"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Counter(\" \".join(df_descrip[\"property_description\"]).split()).most_common(100)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WpAlq6jJ2DpC",
    "outputId": "ba6d7ea0-e0b6-4446-d3e2-550d723373d2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Podemos observar que podr√≠amos optimizar el texto mediante algunas t√©cnicas de reducci√≥n y/o transformaci√≥n. Entre otras:"
   ],
   "metadata": {
    "id": "iwnQ6tgt0MRV"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Eliminemos la etiqueta `<br>` de html:"
   ],
   "metadata": {
    "id": "qK78Ucya0Ugr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df_descrip[\"property_description\"] = df_descrip[\"property_description\"].apply(lambda line: line.replace(\"<br>\", \" \"))\n",
    "df_test_descrip[\"property_description\"] = df_test_descrip[\"property_description\"].apply(lambda line: line.replace(\"<br>\", \" \"))"
   ],
   "metadata": {
    "id": "_o_B51Zk0hGz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transformemos todas las palabras a min√∫sculas, de modo que el contador no realice distinciones:"
   ],
   "metadata": {
    "id": "pyNp4Y520yHS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df_descrip[\"property_description\"] = df_descrip[\"property_description\"].apply(lambda line: line.lower())\n",
    "df_test_descrip[\"property_description\"] = df_test_descrip[\"property_description\"].apply(lambda line: line.lower())\n"
   ],
   "metadata": {
    "id": "VWBmcgfI04Ge"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Quitemos los t√≠ldes de las letras:"
   ],
   "metadata": {
    "id": "WpSPMtks1bZz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df_descrip[\"property_description\"] = df_descrip[\"property_description\"].apply(lambda line: unidecode.unidecode(line))\n",
    "df_test_descrip[\"property_description\"] = df_test_descrip[\"property_description\"].apply(lambda line: unidecode.unidecode(line))"
   ],
   "metadata": {
    "id": "KWHrTmAG1dGn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Eliminemos los s√≠mbolos:"
   ],
   "metadata": {
    "id": "tpUndCZy1j2X"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df_descrip[\"property_description\"] = df_descrip[\"property_description\"].apply(lambda line: re.sub(r'[^\\w]', ' ', line))\n",
    "df_test_descrip[\"property_description\"] = df_test_descrip[\"property_description\"].apply(lambda line: re.sub(r'[^\\w]', ' ', line))"
   ],
   "metadata": {
    "id": "sWcF2SIJ1lfG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Eliminemos los espacios m√∫ltiples entre palabras:"
   ],
   "metadata": {
    "id": "bdqDlvqN1rL1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_descrip[\"property_description\"] = df_descrip[\"property_description\"].apply(lambda line: re.sub(\"\\s\\s+\" , \" \", line))\n",
    "df_test_descrip[\"property_description\"] = df_test_descrip[\"property_description\"].apply(lambda line: re.sub(\"\\s\\s+\" , \" \", line))"
   ],
   "metadata": {
    "id": "uQvajcwC2DpE"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Utilizando el contenido del archivo `stop_words.txt`, eliminemos palabras sin significado del datset y colocamos los cambios en uno nuevo:"
   ],
   "metadata": {
    "id": "BzcWZSJY4GPs"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(stop_words) as f:\n",
    "    lines = f.read().splitlines()\n",
    "\n",
    "f = lambda x: ' '.join([item for item in x.split() if item not in lines])\n",
    "\n",
    "df_descrip[\"property_description\"] = df_descrip[\"property_description\"].apply(f)\n",
    "df_test_descrip[\"property_description\"] = df_test_descrip[\"property_description\"].apply(f)"
   ],
   "metadata": {
    "id": "FrOeCCBY2DpF"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Luego de estas transformaciones, veamos cu√°les son las palabras m√°s utilizadas:"
   ],
   "metadata": {
    "id": "JYXs-85EVQwx"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Counter(\" \".join(df_descrip[\"property_description\"]).split()).most_common(100)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FNYcv0DM2DpG",
    "outputId": "eed3ff96-3911-4d2a-a042-969e76b82e01"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Seleccionemos los aspectos que nos parecen relevantes, para luego buscar sus posibles valores.\n",
    "\n",
    "Para esto, elegimos: `cocina`, `pisos`, `calefaccion`, `expensas`, `lavadero`, `balcon`, `cochera` y `aire` y limpiamos cualquier tipo de formato restante en el dataset:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "Ai_Odw_x2DpG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "aspectos = ['cocina', 'pisos', 'calefaccion', 'expensas', 'lavadero', 'balcon', 'cochera', 'aire']"
   ],
   "metadata": {
    "id": "BkHpD1gtB7e_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for word in aspectos:\n",
    "    df_descrip[word] = df_descrip[\"property_description\"].apply(lambda line: format_aspects(line, word))\n",
    "    df_test_descrip[word] = df_test_descrip[\"property_description\"].apply(lambda line: format_aspects(line, word))"
   ],
   "metadata": {
    "id": "AnaswxtR2DpH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Veamos cu√°les son las 15 palabras m√°s comunes para cada uno de los aspectos elegidos:"
   ],
   "metadata": {
    "id": "EDsk7Bjgx116"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "cant_val_aspectos = 15"
   ],
   "metadata": {
    "id": "eYdLqsvGyNcj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Aspecto `cocina`\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "wWJKdB85y1l0"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Counter(\" \".join(df_descrip[\"cocina\"]).split()).most_common(cant_val_aspectos)"
   ],
   "metadata": {
    "id": "l-7v9FP82DpH",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6bea24da-1ae1-4df4-eb9a-5fddb082398b"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para `cocina`, los valores podr√≠an ser: \n",
    "- integrada\n",
    "- lavadero\n",
    "- completa"
   ],
   "metadata": {
    "collapsed": false,
    "id": "6ACLlV0F2DpI"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Aspecto `pisos`"
   ],
   "metadata": {
    "id": "1XQ4TVyUzSqc"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Counter(\" \".join(df_descrip[\"pisos\"]).split()).most_common(cant_val_aspectos)"
   ],
   "metadata": {
    "id": "UQQMsrnL2DpI",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a261ec0d-9cd6-4277-98b0-7b18d0b28211"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para `pisos`, los valores podr√≠an ser: \n",
    "- porcelanato\n",
    "- parquet\n",
    "- madera"
   ],
   "metadata": {
    "collapsed": false,
    "id": "l39ZahiN2DpJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Aspecto `calefaccion`"
   ],
   "metadata": {
    "id": "a4pG3YbW1wnt"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Counter(\" \".join(df_descrip[\"calefaccion\"]).split()).most_common(cant_val_aspectos)"
   ],
   "metadata": {
    "id": "RPZApHfA2DpJ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "67f4d899-74cf-4d93-ea99-c89946b6f262"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para `calefaccion`, los valores podr√≠an ser: \n",
    "- radiadores\n",
    "- radiante\n",
    "- central\n",
    "- individual"
   ],
   "metadata": {
    "collapsed": false,
    "id": "Mt-7dfsl2DpK"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Aspecto `expensas`"
   ],
   "metadata": {
    "id": "XfzRZc861ysa"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Counter(\" \".join(df_descrip[\"expensas\"]).split()).most_common(cant_val_aspectos*2)"
   ],
   "metadata": {
    "id": "iRKPdGhA2DpK",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e89ec902-f5d3-45e5-df24-45d796fd889b"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para `expensas`, los valores podr√≠an ser: \n",
    "- servicios\n",
    "- impuestos \n",
    "- bajas"
   ],
   "metadata": {
    "collapsed": false,
    "id": "C7EiojY82DpK"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Aspecto `lavadero`"
   ],
   "metadata": {
    "id": "ZVVvuLxu11U0"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Counter(\" \".join(df_descrip[\"lavadero\"]).split()).most_common(cant_val_aspectos)"
   ],
   "metadata": {
    "id": "sAzBKC5c2DpK",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c811e281-914c-4804-fe87-064ada17a12d"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para `lavadero`, los valores podr√≠an ser: \n",
    "- independiente\n",
    "- cocina\n",
    "- comedor"
   ],
   "metadata": {
    "collapsed": false,
    "id": "i8zxU-Co2DpL"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Aspecto `balcon`"
   ],
   "metadata": {
    "id": "0opxhwgx122i"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Counter(\" \".join(df_descrip[\"balcon\"]).split()).most_common(cant_val_aspectos)"
   ],
   "metadata": {
    "id": "geCmhGct2DpL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7f9103b9-03aa-4035-dd78-c3a1c435e833"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para `balcon`, los valores podr√≠an ser: \n",
    "- frente\n",
    "- amplio \n",
    "- terraza \n",
    "- salida \n",
    "- corrido\n",
    "- luminoso"
   ],
   "metadata": {
    "collapsed": false,
    "id": "GUiM00PQ2DpL"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Aspecto `cochera`"
   ],
   "metadata": {
    "id": "W05qxE7f138s"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Counter(\" \".join(df_descrip[\"cochera\"]).split()).most_common(cant_val_aspectos*2)"
   ],
   "metadata": {
    "id": "vF7x7sFV2DpL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "aa1e9494-2db9-48dd-8101-5987d80ccb55"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para `cochera`, los valores podr√≠an ser: \n",
    "- fija\n",
    "- cubierta"
   ],
   "metadata": {
    "collapsed": false,
    "id": "_sGYjhjF2DpM"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Aspecto `aire`"
   ],
   "metadata": {
    "id": "ehAh9leY2Lfu"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Counter(\" \".join(df_descrip[\"aire\"]).split()).most_common(cant_val_aspectos)"
   ],
   "metadata": {
    "id": "eQlqeMRa2DpM",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "fa0a82b5-9f5b-4ab4-80b7-f0090e746727"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para `aire`, posibles valores son: \n",
    "- split \n",
    "- central \n",
    "- acondicionado"
   ],
   "metadata": {
    "collapsed": false,
    "id": "9hWHT90P2DpM"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Consolidaci√≥n de valores"
   ],
   "metadata": {
    "id": "fF4R61si2RJI"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A continuaci√≥n creamos la variable `values`, que contiene los posibles valores para cada uno de los aspectos elegidos:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "ZTu9pPYV2DpN"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "values_cocina = ['integrada' , 'lavadero' , 'completa']\n",
    "values_pisos = ['porcelanato' , 'parquet' , 'madera']\n",
    "values_calefaccion = ['radiadores' , 'radiante' , 'central' , 'individual']\n",
    "values_expensas = ['serviocios' , 'impuestos' , 'bajas']\n",
    "values_lavadero = ['independiente' , 'cocina' , 'comedor']\n",
    "values_balcon = ['frente' , 'amplio' , 'terraza' , 'salida' , 'corrido' , 'luminoso']\n",
    "values_cochera = ['fija' , 'cubierta']\n",
    "values_aire = ['split' , 'central' , 'acondicionado']"
   ],
   "metadata": {
    "id": "8woiCcO32DpN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "values = [values_cocina, values_pisos, values_calefaccion, values_expensas, values_lavadero, values_balcon, values_cochera, values_aire]"
   ],
   "metadata": {
    "id": "CFU_0j0EGUE5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "En primer lugar, creamos un dataset auxiliar que tenga los IDs y las columnas de los aspectos:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "B9JpAzOi2DpN"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "aux_df = df_descrip.copy()\n",
    "aux_df_test = df_test_descrip.copy()\n",
    "\n",
    "aux_df.drop('property_description', inplace=True, axis=1)\n",
    "aux_df_test.drop('property_description', inplace=True, axis=1)"
   ],
   "metadata": {
    "id": "Z617kPUE2DpN"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Luego creamos una funci√≥n a la que -pas√°ndole un dataset, los aspectos y el listado de valores posibles- reemplace el contenido de las columnas por los valores correspondientes."
   ],
   "metadata": {
    "collapsed": false,
    "id": "qpHzefm72DpO"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Modificamos las columnas de los aspectos, para que s√≥lo queden los valores correspondientes:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "8A7lmncH2DpO"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "limpiar_values_de_aspects(aux_df, aspectos, values)\n",
    "limpiar_values_de_aspects(aux_df_test, aspectos, values)\n",
    "aux_df.head(20)"
   ],
   "metadata": {
    "id": "WXhzGxCl2DpO",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "outputId": "3355d0c0-87e8-4707-b349-e4d9b468bfa9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Por √∫ltimo hacemos el merge con el dataset original, teniendo en cuenta los IDs:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "Ia2LxOLq2DpP"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df_train_x.copy()\n",
    "df = pd.merge(df,aux_df, on=\"id\")\n",
    "df.drop(\"id\", inplace=True, axis=\"columns\")\n",
    "\n",
    "df_test = df_test_x.copy()\n",
    "df_test = pd.merge(df_test,aux_df_test, on=\"id\")\n",
    "df_test.drop(\"id\", inplace=True, axis=\"columns\")\n",
    "\n",
    "df.head(20)"
   ],
   "metadata": {
    "id": "I2x-X60t2DpP",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 914
    },
    "outputId": "9aa74e5a-0b67-4010-a31b-a2f67689e136"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exportaci√≥n de Datos"
   ],
   "metadata": {
    "id": "rN3FutIEHrf2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Exportamos los datasets generados:"
   ],
   "metadata": {
    "id": "xd-CoUsZHtL9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if IN_COLAB:\n",
    "  path = '/content/drive/MyDrive/üìî OrganizacioÃÅn de Datos (75.06)/TPS/TP2/DATASETS/1a_df_descrip.csv'\n",
    "else:\n",
    "  path = 'DATASETS/1a_df_descrip.csv'\n",
    "\n",
    "df_descrip.to_csv(path)"
   ],
   "metadata": {
    "id": "qr1jlB8QH16Z"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if IN_COLAB:\n",
    "  path = '/content/drive/MyDrive/üìî OrganizacioÃÅn de Datos (75.06)/TPS/TP2/DATASETS/1a_df_ampliado.csv'\n",
    "else:\n",
    "  path = 'DATASETS/1a_df_ampliado.csv'\n",
    "\n",
    "df.to_csv(path)"
   ],
   "metadata": {
    "id": "QTsLoqr-IHOX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.b Modelos\n",
    "___"
   ],
   "metadata": {
    "collapsed": false,
    "id": "fI1Fi95S2DpP"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Sin optimizaci√≥n de hiperpar√°metros"
   ],
   "metadata": {
    "id": "ldOrNGnEaE5H"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Entrenemos un modelo de XGBoost con los mismos hiperpar√°metros utilizados en el TP1."
   ],
   "metadata": {
    "id": "a2X0YkeUUL1X"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Realizamos One Hot Encoding para las nuevas variables cualitativas:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "WqUMr9_r2DpQ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_dummies = pd.get_dummies(df, columns=['cocina', 'pisos', 'lavadero', 'calefaccion', 'expensas', 'balcon', 'cochera', 'aire'], drop_first=True)\n",
    "df_test_dummies = pd.get_dummies(df_test, columns=['cocina', 'pisos', 'lavadero', 'calefaccion', 'expensas', 'balcon', 'cochera', 'aire'], drop_first=True)\n",
    "df_dummies.head(5)"
   ],
   "metadata": {
    "id": "5e0r2Yj-2DpQ",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "outputId": "9345f93a-6a15-48ed-ebd9-8004cb87637f"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_dummies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Entrenamos el modelo:"
   ],
   "metadata": {
    "id": "I6cp1CI3ZKLx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df_test_dummies"
   ],
   "metadata": {
    "id": "kQ5AadFSfxhE",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "044f7dba-5555-4d62-b84e-494ca9febd99"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "XGB_Regressor = XGBRegressor(min_child_weight = 5, max_depth = 6, learning_rate = 0.3, gamma = 0.1, colsample_bytree = 0.3)\n",
    "XGB_Regressor.fit(df_dummies, df_train_y_regresion)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hagamos las predicciones y veamos c√≥mo resultaron las m√©tricas del modelo:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediccion_y_metricas_regresion(XGB_Regressor,df_test_dummies, df_test_y_regresion)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Importancia de features\n",
    "Graficamos los 15 features m√°s importantes para el modelo con los hiperpar√°metros del TP1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sorted_idx = XGB_Regressor.feature_importances_.argsort()\n",
    "sorted_idx = sorted_idx[-15:]\n",
    "plt.barh(df_test_dummies.columns[sorted_idx], XGB_Regressor.feature_importances_[sorted_idx])\n",
    "plt.xlabel(\"Xgboost Feature Importance\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Podemos observar que el feature m√°s importante es la ubicaci√≥n en Puerto Madero, seguido de cantidad de habitaciones y ubicaci√≥n en Palermo Chico."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Con optimizaci√≥n de hiperpar√°metros"
   ],
   "metadata": {
    "id": "CkJCoGXnaN1y"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Veamos c√≥mo se comporta el score con la optimizaci√≥n de hiperpar√°metros:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "Su2wKojB2DpR"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "params_grid = {'learning_rate': [0.20, 0.25, 0.30],\n",
    "               'max_depth': [4, 5, 6, 8, 10],\n",
    "               'min_child_weight': [1, 3, 5, 7, 9],\n",
    "               'gamma': [0.1, 0.2 , 0.3, 0.4, 0.5],\n",
    "               'colsample_bytree' : [0.3, 0.4, 0.5, 0.7, 0.8]}\n",
    "\n",
    "randomCV = RandomizedSearchCV(estimator = XGBRegressor(),\n",
    "                              param_distributions = params_grid,\n",
    "                              scoring = make_scorer(r2_score),\n",
    "                              cv = StratifiedKFold(n_splits = 5),\n",
    "                              n_iter = 5)\n",
    "\n",
    "randomCV.fit(df_dummies, df_train_y_regresion)"
   ],
   "metadata": {
    "id": "7RTpJeq92DpS",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c628a9ba-c22a-41d7-fe70-c2cac713f798"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "randomCV.best_params_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediccion_y_metricas_regresion(randomCV.best_estimator_,df_test_dummies, df_test_y_regresion)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Observamos que, si bien las m√©tricas mejoraron, las diferencias al optimizar los par√°metros no fueron muy significativas."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Importancia de features\n",
    "Graficamos los 15 features m√°s importantes para el modelo con los hiperpar√°metros optimizados"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sorted_idx = randomCV.best_estimator_.feature_importances_.argsort()\n",
    "sorted_idx = sorted_idx[-15:]\n",
    "plt.barh(df_test_dummies.columns[sorted_idx], randomCV.best_estimator_.feature_importances_[sorted_idx])\n",
    "plt.xlabel(\"Xgboost Feature Importance\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Podemos ver que la ubicaci√≥n en Puerto Madero tiene a√∫n m√°s peso que en el modelo anterior y tambi√©n como algunos de los features con los que expandimos el dataset (calefaccion_radiante y cochera_fija) toman m√°s protagonismo, al igual que la superficie total."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exportaci√≥n del Modelo\n",
    "Finalmente exportamos el modelo utilizado para predecir, resultante de la optimizaci√≥n de hiperpar√°metros:"
   ],
   "metadata": {
    "id": "_f0JxWEL8GUY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if IN_COLAB:\n",
    "  path = '/content/drive/MyDrive/üìî OrganizacioÃÅn de Datos (75.06)/TPS/TP2/MODELOS/XGB_Regressor.json'\n",
    "else:\n",
    "  path = './MODELOS/XGB_Regressor.json'\n",
    "\n",
    "randomCV.best_estimator_.save_model(path)"
   ],
   "metadata": {
    "id": "RNVI92fe8WEP",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9e1b9542-5eec-4f40-edbc-ebf7c8cecc42"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Redes Neuronales"
   ],
   "metadata": {
    "collapsed": false,
    "id": "HWpWT3GZ_TQT"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.a Regresi√≥n"
   ],
   "metadata": {
    "id": "2HjJDe4W_iY-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.a.1 Preparaci√≥n del dataset"
   ],
   "metadata": {
    "id": "5xI2H0xyYm_B"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train_regresion = df_train_x.drop([\"id\"], axis=1).copy()\n",
    "y_train_regresion = df_train_y_regresion.copy()\n",
    "x_test_regresion = df_test_x.drop([\"id\"], axis=1).copy()\n",
    "y_test_regresion = df_test_y_regresion.copy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Normalizamos las entradas con StandardScaler:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "PKPRQVFS_TQT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x_train_regresion[['start_date', 'end_date', 'latitud', 'longitud', 'property_rooms', 'property_surface_total']] = zscore(x_train_regresion[['start_date', 'end_date', 'latitud', 'longitud', 'property_rooms', 'property_surface_total']], axis=1)\n",
    "\n",
    "x_test_regresion[['start_date', 'end_date', 'latitud', 'longitud', 'property_rooms', 'property_surface_total']] = zscore(x_test_regresion[['start_date', 'end_date', 'latitud', 'longitud', 'property_rooms', 'property_surface_total']], axis=1)"
   ],
   "metadata": {
    "id": "s3xTEOkRlufG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.a.2 B√∫squeda del mejor modelo"
   ],
   "metadata": {
    "id": "NjqmQr4pYrEE"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creo una funci√≥n que me permite generar un modelo a partir de sus hiperpar√°metros. Esta funci√≥n tiene como par√°metros la cantidad de nodos de la primera y ante√∫ltima capa, la cantidad de capas ocultas, la funci√≥n de activaci√≥n y el optimizador. Todos los modelos que genera a excepci√≥n de los casos sin capas ocultas) tienen forma de 'pir√°mide'."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cantidad_de_columnas = x_train_regresion.shape[1]\n",
    "\n",
    "def crear_modelo(hidden_layers, first_layer_nodes, last_layer_nodes, activation_func, optimizer):\n",
    "\n",
    "    sequential = Sequential()\n",
    "    sequential.add(keras.layers.Dense(first_layer_nodes, input_shape=(cantidad_de_columnas,), activation='relu'))\n",
    "\n",
    "    if hidden_layers is 0 or hidden_layers is 1:\n",
    "        decremento = 0\n",
    "    else:\n",
    "        decremento = math.ceil((first_layer_nodes - last_layer_nodes) / (hidden_layers - 1))\n",
    "\n",
    "    for i in range (0, hidden_layers):\n",
    "        nodos = first_layer_nodes - decremento * i\n",
    "        sequential.add(Dense(nodos, activation=activation_func))\n",
    "\n",
    "    sequential.add(Dense(1, activation='linear'))\n",
    "\n",
    "    sequential.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='mse',\n",
    "        metrics=['mse', 'mean_absolute_percentage_error']\n",
    "    )\n",
    "\n",
    "    return sequential\n",
    "\n",
    "\n",
    "modelo =  KerasRegressor(build_fn=crear_modelo, verbose = False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Luego busco el mejor modelo a partir de una grilla de par√°metros arbitrarios con el m√©todo de 'GridSearchCV'. El criterio de mejor modelo es el que tenga menor error cuadrado, o lo que es equivalente, mayor error cuadrado negado. Nos limitamos en la cantidad de folds en el CV por el consumo temporal."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "param_grid = dict(\n",
    "    hidden_layers=[2, 4],\n",
    "    first_layer_nodes = [x_train_regresion.shape[1], math.ceil(cantidad_de_columnas * (2/3))],\n",
    "    last_layer_nodes = [5],\n",
    "    activation_func = ['sigmoid', 'relu'],\n",
    "    batch_size = [750],\n",
    "    epochs = [70],\n",
    "    optimizer=['RMSprop'],\n",
    ")\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator = modelo,\n",
    "    param_grid = param_grid,\n",
    "    cv=5,\n",
    "    error_score='raise',\n",
    "    scoring='neg_mean_squared_error',\n",
    "    verbose=0,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Entrenamos todos los modelos"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid.fit(x_train_regresion, y_train_regresion)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Guardamos el historial de entrenamiento del mejor modelo"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history = grid.best_estimator_.model.history.history"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Los par√°metros y m√©tricas del mejor modelo fueron:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"El error absoluto porcentual promedio del mejor modelo fue de: \", grid.best_estimator_.model.metrics[2].result().numpy())\n",
    "print(\"El error absoluto cuadrado promedio del mejor modelo fue de: \", grid.best_estimator_.model.metrics[1].result().numpy())\n",
    "print(\"Los par√°metros seleccionados por Grid search 5-fold cross validation fueron: \", grid.best_params_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.a.3 Predicci√≥n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = grid.predict(x_test_regresion)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.a.4 M√©tricas"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Veamos c√≥mo se comporta la funci√≥n de p√©rdida del mejor modelo seg√∫n la cantidad de √©pocas utilizadas:"
   ],
   "metadata": {
    "id": "vmBg6ev_ZO-e"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = range(70)\n",
    "plt.plot(epochs, history['mse'], color='orange', label='MSE')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title('Error cuadr√°tico medio por cantidad de √©pocas')\n",
    "plt.legend()"
   ],
   "metadata": {
    "id": "BzNqiJex_TQU",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "outputId": "214e1825-e3d1-4d66-f0db-9b0cdf7a2550"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mse = metrics.mean_squared_error(\n",
    "    y_true  = y_test_regresion,\n",
    "    y_pred  = y_pred,\n",
    "    squared = True\n",
    ")\n",
    "\n",
    "print(f\"El error seg√∫n la m√©trica 'Mean Square Error' de test es: {mse}\")\n",
    "\n",
    "rmse = metrics.mean_squared_error(\n",
    "    y_true  = y_test_regresion,\n",
    "    y_pred  = y_pred,\n",
    "    squared = False\n",
    ")\n",
    "\n",
    "print(f\"El error seg√∫n la m√©trica 'Root Mean Square Error' de test es: {rmse}\")\n",
    "\n",
    "r2 = metrics.r2_score(\n",
    "    y_true  = y_test_regresion,\n",
    "    y_pred  = y_pred\n",
    ")\n",
    "\n",
    "print(f\"El error seg√∫n la m√©trica 'R2 Score' de test es: {r2}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.a.5 Exportaci√≥n de Datos"
   ],
   "metadata": {
    "id": "BeRZVagKeKzO"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finalmente, exportamos el modelo utilizado:"
   ],
   "metadata": {
    "id": "lO3v9OUzeKzP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if IN_COLAB:\n",
    "  path = '/content/drive/MyDrive/üìî OrganizacioÃÅn de Datos (75.06)/TPS/TP2/MODELOS/Redes_Regressor.joblib'\n",
    "else:\n",
    "  path = './MODELOS/Redes_Regressor.joblib'\n",
    "\n",
    "joblib.dump(grid.best_estimator_.model, path)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ac67727c-79b0-4c22-bb79-7bd9362015f4",
    "id": "tSuqGenveKzP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.b Clasificaci√≥n\n",
    "___"
   ],
   "metadata": {
    "collapsed": false,
    "id": "dXdWW-m9_TQV"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.b.1 Preparaci√≥n del dataset"
   ],
   "metadata": {
    "id": "b176QcPLZj9N"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train_clasificacion = df_train_x.drop([\"id\"], axis=1).copy()\n",
    "y_train_clasificacion = df_train_y_clasificacion.copy()\n",
    "x_test_clasificacion = df_test_x.drop([\"id\"], axis=1).copy()\n",
    "y_test_clasificacion = df_test_y_clasificacion.copy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Normalizamos mediante Z-Score:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "JPjRt6tTOWIL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x_train_clasificacion[['start_date', 'end_date', 'latitud', 'longitud', 'property_rooms', 'property_surface_total']] = zscore(x_train_clasificacion[['start_date', 'end_date', 'latitud', 'longitud', 'property_rooms', 'property_surface_total']], axis=1)\n",
    "\n",
    "x_test_clasificacion[['start_date', 'end_date', 'latitud', 'longitud', 'property_rooms', 'property_surface_total']] = zscore(x_test_clasificacion[['start_date', 'end_date', 'latitud', 'longitud', 'property_rooms', 'property_surface_total']], axis=1)"
   ],
   "metadata": {
    "id": "qDlv11yi8sR1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Aplicamos One Hot encoding a la columna target de entrenamiento y test, para que tenga 3 columnas al igual que la salida del modelo:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "IZ2XeEKi_TQW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "y_train_clasificacion = pd.get_dummies(y_train_clasificacion, columns=['tipo_precio_3'], drop_first=False)\n",
    "y_test_clasificacion = pd.get_dummies(y_test_clasificacion, columns=['tipo_precio_3'], drop_first=False)"
   ],
   "metadata": {
    "id": "2jzMgy7b5js1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.b.2 B√∫squeda del mejor modelo"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Funci√≥n auxiliar para que GridSearchCV realice el scoring de forma correcta."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def my_categorical_accuracy(y_true, y_pred) :\n",
    "    y_pred_df = pd.DataFrame(data=y_pred, columns=['alto', 'bajo', 'medio'])\n",
    "    res = round(accuracy_score(y_true, y_pred_df), 2)\n",
    "    return res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creamos una funci√≥n que permite generar un modelo a partir de sus hiperpar√°metros configurables.\n",
    "Esta funci√≥n recibe como par√°metros la cantidad de capas ocultas extra, la cantidad de nodos de la √∫ltima capa oculta, la funci√≥n de activaci√≥n y metadata del clasificador."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def crear_modelo(extra_hidden_layers, last_layer_nodes, activation_func, loss_func, meta):\n",
    "\n",
    "    n_features_in_ = meta[\"n_features_in_\"]\n",
    "    X_shape_ = meta[\"X_shape_\"]\n",
    "    n_classes_ = meta[\"n_classes_\"]\n",
    "\n",
    "    sequential = Sequential()\n",
    "    sequential.add(keras.layers.Dense(n_features_in_ * 0.7, input_shape=X_shape_[1:], activation='relu'))\n",
    "\n",
    "    if last_layer_nodes > n_features_in_:\n",
    "        decremento = 0\n",
    "    elif extra_hidden_layers is 0 or extra_hidden_layers is 1:\n",
    "        decremento = 0\n",
    "    else:\n",
    "        decremento = math.ceil((n_features_in_ - last_layer_nodes) / (extra_hidden_layers - 1))\n",
    "\n",
    "    for i in range (0, extra_hidden_layers):\n",
    "        nodos = n_features_in_ - decremento * i\n",
    "        sequential.add(Dense(nodos, activation=activation_func))\n",
    "\n",
    "    sequential.add(Dense(n_classes_, activation='softmax'))\n",
    "\n",
    "    sequential.compile(\n",
    "        loss=loss_func\n",
    "    )\n",
    "\n",
    "    return sequential\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creamos un modelo base, necesario seg√∫n la documentaci√≥n de SciKeras - KerasClassifier.\n",
    "Este modelo base ser√° editado por GridSearchCV al buscar los hiperpar√°metros."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "base_model = KerasClassifier(\n",
    "    crear_modelo,\n",
    "    loss_func=\"categorical_crossentropy\",\n",
    "    extra_hidden_layers=1,\n",
    "    last_layer_nodes=1,\n",
    "    activation_func='sigmoid',\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Definimos los hiperpar√°metros posibles. Adem√°s de los necesarios por la funci√≥n crear_modelo, agregamos diferentes optimizadores, learning rates y cantidad de √©pocas."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "param_grid = dict(\n",
    "    extra_hidden_layers=[1, 4],\n",
    "    last_layer_nodes = [5, 10, 20],\n",
    "    activation_func = ['relu', 'softmax'],\n",
    "    batch_size = [750],\n",
    "    epochs = [20],\n",
    "    optimizer__learning_rate = [0.001],\n",
    "    optimizer = [\"adam\", \"sge\"],\n",
    "    loss_func = [\"categorical_crossentropy\"],\n",
    ")\n",
    "\n",
    "gs = GridSearchCV(base_model,\n",
    "                  param_grid = param_grid,\n",
    "                  cv=5,\n",
    "                  scoring=make_scorer(my_categorical_accuracy),\n",
    "                  verbose=0,\n",
    "                  error_score='raise'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Entrenamos los diferentes modelos y obtenemos los mejores par√°metros."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gs.fit(x_train_clasificacion, y_train_clasificacion)\n",
    "print(\"Los par√°metros √≥ptimizados fueron: \", gs.best_params_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Predecimos con el dataset de test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pr = gs.predict(x_test_clasificacion)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.b.4 M√©tricas\n",
    "Transformamos el array devuelto por el modelo a un dataframe de iguales columnas que y_test_clasificacion"
   ],
   "metadata": {
    "id": "W-KMjjHwaFjg"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pr_df = pd.DataFrame(data=y_pr, columns=['alto', 'bajo', 'medio'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calculamos las m√©tricas"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metricas_clasificacion(y_test_clasificacion, y_pr_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.b.5 Exportaci√≥n de Datos"
   ],
   "metadata": {
    "id": "5SsWSPw1fJYm"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finalmente, exportamos el modelo utilizado:"
   ],
   "metadata": {
    "id": "IlgF2MmmfJYn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "\n",
    "if IN_COLAB:\n",
    "  path = '/content/drive/MyDrive/üìî OrganizacioÃÅn de Datos (75.06)/TPS/TP2/MODELOS/Redes_Classifier.joblib'\n",
    "else:\n",
    "  path = './MODELOS/Redes_Classifier.joblib'\n",
    "\n",
    "joblib.dump(gs.best_estimator_, path)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "48479ec0-085c-4964-aa1d-b771dc5643e4",
    "id": "yjKFj8TjfJYn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDpREdwKxB6m"
   },
   "source": [
    "## 3. Ensamble de Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvLq10beDadg"
   },
   "source": [
    "### 3.1 Ensamble H√≠brido: Voting - Clasificaci√≥n\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAT6HU3buhLU"
   },
   "source": [
    "#### 3.1.1 Preparaci√≥n del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZNjL2W-vZvC"
   },
   "source": [
    "Para la parte de ensambles, lo que haremos ser√° utilizar nuevamente el dataset al cual se le aplic√≥ una reducci√≥n de su dimensionalidad en el trabajo pr√°ctico n¬∞1. \n",
    "\n",
    "Para esto lo que haremos ser√° trabajar con una copia del dataset modificado al inicio del trabajo, el cual usa como base el reducido mencionado anteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDp1eW_cHfxt"
   },
   "source": [
    "Para nuestra variable `target`, utilizaremos como convenci√≥n la misma que fue planteada para el tp1. √âsta consiste en subdividir a la variable pxm2 (precio por metro cuadrado) en 3 intervalos, 25% a bajo, 50% a medio y el otro 25% restante a alto. A su vez, se har√° la separaci√≥n tambien por tipo de propiedad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7T0ZW9l-dcqI"
   },
   "outputs": [],
   "source": [
    "x_train_voting = x_train_clasificacion\n",
    "y_train_voting = y_train_clasificacion\n",
    "\n",
    "x_test_voting = x_test_clasificacion\n",
    "y_test_voting = y_test_clasificacion"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "A modo de finalizaci√≥n de este trabajo, haremos un peque√±o repaso sobre todos los puntos que analizamos y sus respectivos resultados y/o observaciones hechos por nosotros:\n",
    "\n",
    "Sobre el an√°lisis realizado en el apartado de **procesamiento del lenguaje natural**, y en particular haciendo referencia a las m√©tricas que obtuvimos luego del entrenamiento y predicci√≥n con el dataset producto de la ampliaci√≥n, utilizando el modelo XGBoost, pudimos observar que si bien a√∫n las m√©tricas no nos resultan del todo satisfactorias cumplen con el objetivo de mejorar las resultantes del trabajo anterior. M√°s all√° de que la cantidad de aciertos contin√∫a sin incrementar, tanto el error MSE como el RMSE decrecieron ofreciendo mejores resultados. Por otro lado, el coeficiente de determinaci√≥n se increment√≥ notoriamente llegando casi a un 0.9%. \n",
    "\n",
    "En cuanto a lo referido sobre **redes neuronales**, la comparaci√≥n ser√° directamente en base a los nuevos modelos ya que el dataset utilizado es compartido en ambos trabajos. Dicho esto, el error cometido en el modelo de regresi√≥n es significativamente peque√±o. Teniendo en cuenta los resultantes del TP1, si utilizamos el menor luego de que √©stos tuvieron su optimizaci√≥n de par√°metros correspondiente, notamos que el que mejor performa es XGBoost con un MSE de 51650994876 (al cual de todas formas le atribu√≠mos arrastre de error en nuestro planteo). Utilizando redes neuronales cometemos tan solo un error cuadr√°tico medio equivalente a 1.36. Por otro lado, en lo que respecta al modelo de clasificaci√≥n, obtuvimos resultados bastante similares. Si bien tanto el accuracy, precisi√≥n y F1-Score dieron distintos, la diferencia es de tan solo uno/dos puntos. Con esto concluimos que para clasificaci√≥n no se notan mejoras por sobre los modelos utilizados en el trabajo anterior.\n",
    "\n",
    "Como √∫ltimo requerimiento se pidi√≥ estudiar las m√©tricas tanto para regresi√≥n como clasificaci√≥n pero haciendo uso de **ensambles**, en particular de tipo h√≠bridos. En lo que respecta al utilizado para clasificaci√≥n (Voting) obtuvimos mejores resultados para las m√©tricas de Accuracy, Recall y F1-Score. Sin embargo, aunque en Precisi√≥n disminuy√≥ un poco, esta diferencia contin√∫a siendo no significativa. Pasando al ensamble de regresi√≥n (Stacking) el MSE resultante que obtuvimos fue 238481755830. Creemos que nuevamente estamos cometiendo involuntariamente alg√∫n arrastre de error en alguno de los incisos desarrollados, ya que este error es incluso mayor a los cometidos en los modelos individuales desarrollados en el TP1, y no estar√≠a respetando la regla principal de ‚Äò*La sabidur√≠a de las multitudes*‚Äô."
   ],
   "metadata": {
    "id": "CE40zH4PI4xY"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bajo = y_train_voting['bajo'].sum() + y_test_voting[y_test_voting == 'bajo'].count()\n",
    "medio = y_train_voting['medio'].sum() + y_test_voting[y_test_voting == 'medio'].count()\n",
    "alto = y_train_voting['alto'].sum() + y_test_voting[y_test_voting == 'alto'].count()\n",
    "\n",
    "print(f\"Se observaron: \\n - {round(bajo,3)} registros de tipo 'bajo'. \\n - {round(medio,3)} registros de tipo 'medio'. \\n - {round(alto,3)} registros de tipo 'alto'.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.1.2 Definici√≥n del Ensamble"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para el tipo de ensamble **voting**, lo que necesitaremos ser√° contar con `n` cantidad de modelos previamente entrenados para luego someterlos a una votaci√≥n. De la misma, saldr√° la clasificaci√≥n para la nueva instancia en base a lo que indique la mayor√≠a de ellos."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Elegimos tomar como modelos los mismos empleados en el TP1:\n",
    "\n",
    "\n",
    "*   √Årbol de Decisi√≥n\n",
    "*   Random Forest\n",
    "*   KNN\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dcs_clf = DecisionTreeClassifier()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "knn_clf = KNeighborsClassifier()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Una vez que contamos con los modelos que vamos a utilizar en el ensamble, procedemos a su creaci√≥n. En este caso particular decidimos utilizar el tipo de votaci√≥n hard el cual utilizar√° la regla de la mayor√≠a. Por otro lado, utilizamos el hiperpar√°metro `estimators` para definir como nos vamos a referir a dichos modelos."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vot_clf = VotingClassifier(estimators = [('dcs', dcs_clf), ('rnd', rnd_clf), ('knn', knn_clf)], voting = 'hard')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.1.3 Entrenamiento y Predicci√≥n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Reorganizamos los valores predichos para que queden en una sola columna, y comparamos con el conjunto de prueba:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_train_voting_arr = np.apply_along_axis(convert_b_m_a, axis=1, arr=y_train_voting)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Seguimos reacomodando los valores para poder calcular las m√©tricas correspondientes, notemos que no pueden guardarse valores equivalentes al string 'medio' en el array:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.unique(y_train_voting_arr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Aplicamos estrategias de transformaci√≥n de datos:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_train_voting_arr = np.where(y_train_voting_arr == 'medi', 'medio', y_train_voting_arr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.unique(y_train_voting_arr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finalmente, entrenamos el ensamle:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vot_clf.fit(x_train_voting, y_train_voting_arr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred_voting = vot_clf.predict(x_test_voting)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.1.4 M√©tricas"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para poder determinar que tan bueno resulto el modelo, lo que haremos ser√° observar las `m√©tricas` resultantes de una predicci√≥n con los datos de test. Recordemos que nuestras funcinoes de metricas fueron definidas al inicio de este trabajo."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metricas_clasificacion(y_test_voting, y_pred_voting)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A su vez, tambi√©n podemos visualizar los mismos a trav√©s de la siguiente matriz de confusi√≥n."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "matrix = confusion_matrix(y_test_voting, y_pred_voting)\n",
    "sns.heatmap(matrix,cmap='GnBu',annot=True,fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.1.4 Exportaci√≥n de Datos"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finalmente, exportamos el modelo utilizado:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "  path = '/content/drive/MyDrive/üìî OrganizacioÃÅn de Datos (75.06)/TPS/TP2/MODELOS/Voting_Classifier.joblib'\n",
    "else:\n",
    "  path = './MODELOS/Voting_Classifier.joblib'\n",
    "\n",
    "dump(vot_clf, path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 Ensamble H√≠brido: Stacking - Regresi√≥n\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.1.1 Preparaci√≥n del dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nuevamente, utilizamos los datasets de train y test previamente separados:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train_stacking = x_train_regresion\n",
    "y_train_stacking = y_train_regresion\n",
    "\n",
    "x_test_stacking = x_test_regresion\n",
    "y_test_stacking = y_test_regresion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2.2 Definici√≥n del Ensamble\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lo que haremos en esta nueva secci√≥n, ser√° implementar un nuevo tipo de ensable h√≠brido con la salvedad de que esta vez utilizaremos el tipo `cascading`.\n",
    "El mismo se basa en el entrenamiento de distintos `modelos base`, y a su vez utilizar√° un `meta-modelo` el cual realizar√° su predicci√≥n en base a las predicciones de los diferentes modelos comentados anteriormente.\n",
    "\n",
    "Como es indicado por el enunciado del trabajo se utilizar√°n modelos de regresi√≥n. En particular, decidimos trabajar con:\n",
    "\n",
    "\n",
    "*   KNN\n",
    "*   XGBoost\n",
    "*   AdaBoost\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "knn_rgs = KNeighborsRegressor()\n",
    "xgb_rgs = XGBRegressor()\n",
    "adb_rgs = AdaBoostRegressor()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Luego utilizamos los mismos para definir nuestro modelo base, en el cual luego se basar√° el meta-modelo."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "base_models = [('KNN', knn_rgs),\n",
    "               ('XGBoost', xgb_rgs),\n",
    "               ('AdaBoost', adb_rgs)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Como mencionamos, a continuaci√≥n los utilizaremos para definir nuestro meta-modelo. Para este √∫ltimo, decidimos emplear el modelo de regresi√≥n logistica."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "meta_model = GradientBoostingRegressor()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creamos nuestro ensamble indicando como modelos estimadores Knn, XGBoost y AdaBoost, y como estimador final el modelo de Regresi√≥n Lineal."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "s = 5\n",
    "n = 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stacking_model = StackingRegressor(estimators=base_models,\n",
    "                                    final_estimator=meta_model,\n",
    "                                    passthrough=True,\n",
    "                                    cv=s,\n",
    "                                    verbose=n)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2.3 Entrenamiento y Predicci√≥n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_scores = defaultdict()\n",
    "\n",
    "for name, model in base_models:\n",
    "    print('Evaluating {}'.format(name))\n",
    "    scores = evaluate_model(model, x_train_stacking, y_train_stacking, s, n)\n",
    "    model_scores[name] = scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Veamos c√≥mo resultaron los scores de los modelos:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_results(model_scores, name='stacking_model_cv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Entrenamos el modelo:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stacking_model.fit(x_train_stacking, y_train_stacking)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finalmente haremos la predicci√≥n:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred_stacking = stacking_model.predict(x_test_stacking)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2.4 M√©tricas"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para poder evaluar la performance que obtuvo nuestro modelo utilizaremos la m√©trica de evaluaci√≥n del error cuadr√°tico medio (MSE) como se pide por enunciado."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mse = metrics.mean_squared_error(\n",
    "        y_true  = y_test_stacking,\n",
    "        y_pred  = y_pred_stacking,\n",
    "        squared = True\n",
    "       )\n",
    "\n",
    "print(f\"El error seg√∫n la m√©trica 'Mean Square Error' de test es: {mse}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2.4 Exportaci√≥n de Datos"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finalmente, exportamos el modelo utilizado:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "  path = '/content/drive/MyDrive/üìî OrganizacioÃÅn de Datos (75.06)/TPS/TP2/MODELOS/Stacking_Regressor.joblib'\n",
    "else:\n",
    "  path = './MODELOS/Stacking_Regressor.joblib'\n",
    "\n",
    "dump(stacking_model, path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusiones"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "EAT6HU3buhLU",
    "_kG7ZRlKuaxT",
    "D8sgMCouvLx-"
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
